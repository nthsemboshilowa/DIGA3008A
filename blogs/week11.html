<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NTSHEMBO SHILOWA, Week 1 Blog</title>
    <meta name="author" content="Week 1 blog content about getting started with web development and Github and Github pages ">
    <meta name="description" content="Blogs">

    <link rel="stylesheet" href="../styles/main.css">
    <link rel="stylesheet" href="../styles/blogcontent.css">

</head>

<body>
    <header>
        <nav>
            <a href="../index.html">Home</a>
            <a href="../blogs/index.html">Blogs</a>
            <a href="../design/index.html">Design</a>
            <a href="../essays/index.html">Essays</a>
            <a href="../portfolio/index.html">Portfolio</a>
        </nav>
    </header>
    <main>
        <article class="blog-article">
          <h1>Blog 11</h1>
          
          <div>
            <h2>Digital inequalities in the Age of Artificial Intelligence and Big Data (Lutz, 2019)</h2>
            
            <div>

              <p>
                In Digital Inequalities in the Age of Artificial Intelligence, Christoph Lutz (2019) explains how artificial intelligence (AI) can deepen digital inequalities. He contends that AI technologies are often created and utilized to serve powerful groups while disadvantaging or marginalizing others. Instead of closing the digital divide, AI risks making it worse. This happens through biased algorithms, uneven access to AI tools, and the dominance of large technology companies that shape how AI is used.
              </p>
<p>
  Lutz (2019) shows that AI systems are not neutral or fair. They reflect the goals and values of the people who design them, most of whom come from wealthy countries and large corporations. Because of this, AI often works better for people in the Global North and fails to consider the needs of others. For example, facial recognition software may not recognize darker skin tones as it would lighter ones, which shows how bias in data can lead to unfair results. Lutz also warns that AI makes decisions in areas like policing, hiring, and healthcare, where unfair treatment can have significant effects.
</p>

<p>This view fits with the idea of <b>digital justice</b>, which means everyone should have equal access to digital tools and a say in their use. Lutz (2019) supports this idea by calling for more digital literacy, critical thinking, and public participation in AI decision-making. He argues that AI should not only be controlled by tech companies and governments but involve a broader range of voices, especially from communities often left out.
</p>

<p>
  Another critical voice in this discussion is Virginia Eubanks. Her book Automating Inequality (2018) shows how AI and data systems often target poor people unfairly. For example, welfare systems in the United States have used automated tools to decide who deserves help. People were often wrongly removed from these programs, causing harm. Eubanks (2018) argues that these systems treat poverty as a problem that must be controlled rather than a condition that needs support. Like Lutz, she believes technology is often used to support existing power structures, not fix them.
</p>

<p>
  From a <b>postcolonial perspective</b>, both Lutz and Eubanks show how AI continues old patterns of inequality. Most AI development occurs in the Global North, while data and labour are often taken from the Global South without proper benefits or consent. This creates a situation where rich countries gain the most from AI while poorer countries are left out or harmed. This kind of digital colonialism means that the same unequal relationships from the past are repeated in new ways.
</p>

<p>
  From the viewpoint of <b>ethical design</b>, these issues show that fixing AI bias is not just about changing algorithms. Ethical design means including people from different backgrounds in the design process, being transparent about decisions made and ensuring that AI systems serve the public good. Lutz (2019) and Eubanks (2018) argue that fairness, inclusion, and accountability must be built into how AI systems are created and used.
</p>

              <p>
                In conclusion, AI is not just a tool but a powerful force that can shape society. If left unchecked, it can increase inequality and reinforce unfair systems. The work of Lutz (2019) and Eubanks (2018) helps us see the risks. Their ideas support critical frameworks like digital justice, postcolonial thinking, and ethical design. As AI continues to grow, it is essential that we use it to support equality and human rights—not just the interests of the most powerful.
              </p>
<h3>References</h3>
                <p>
                  Eubanks, V., 2018. Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor. New York: St. Martin’s Press.
                  </p>
                  <p>
                  Lutz, C., 2019. Digital inequalities in the age of artificial intelligence and big data. Human Behavior and Emerging Technologies, 1(2), pp.141-148.
                </p>
            </div>
          </div>
        </article>
        <a href="../blogs/index.html" class="back-to-blog-fixed">← Back to Blog</a>
      </main>

</body>
</html>